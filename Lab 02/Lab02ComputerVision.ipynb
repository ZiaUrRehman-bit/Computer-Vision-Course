{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **<center><font style=\"color:rgb(100,109,254)\">Digital Image Processing</font> </center>**\n",
    "### **<center><font style=\"color:rgb(100,109,254)\">Lab 02</font> </center>**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "****"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **<font style=\"color:rgb(255,0,255)\">01. Convert BGR image to Grayscale Without using cvtColor Function</font>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"C:\\\\Users\\\\hp\\\\Google Drive\\\\Fiverr Work\\\\2022\\\\33. Computer Vision Course\\\\pictures\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = cv2.imread(path+\"\\\\coloredChips.png\")  # took path and name of image as an argument"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1.1 Split the channles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "blueChannel = img[:,:,0]\n",
    "greenChannel = img[:,:,1]\n",
    "redChannel = img[:,:,2]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1.2 Convert it to Grayscale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "grayImg = (0.299 * redChannel + 0.587 * greenChannel + 0.114 * blueChannel)/255"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **<font style=\"color:rgb(255,0,0)\">Why we need to multiply these values with each cahnnel?</font>**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When converting a BGR image to grayscale, we need to take into account the different contributions of each color channel to the perceived brightness of a pixel.\n",
    "\n",
    "The human eye is more sensitive to green light than red or blue, so we should assign a higher weight to the green channel in the grayscale conversion formula. The coefficients 0.299, 0.587, and 0.114 that we use in the formula are based on the relative luminance of each color channel, taking into account the sensitivity of the human eye to each color.\n",
    "\n",
    "Therefore, we multiply each color channel by its corresponding coefficient to compute the grayscale value for each pixel. This ensures that the resulting grayscale image accurately represents the relative brightness of the original color image, while also taking into account the human eye's sensitivity to different colors."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **<font style=\"color:rgb(255,0,0)\">Why these specific values? where they come from?</font>**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The values 0.299, 0.587, and 0.114 that are commonly used in the grayscale conversion formula for BGR images are based on the relative luminance of each color channel, as well as the sensitivity of the human eye to different colors.\n",
    "\n",
    "The coefficients were originally derived from the CIE (Commission Internationale de l'Eclairage) color space, which defines the standard observer model for the human eye. The CIE model includes a set of color matching functions that describe the spectral sensitivity of the human eye to different colors.\n",
    "\n",
    "The coefficients used in the grayscale conversion formula are based on the CIE color matching functions, and are designed to approximate the perceived brightness of a color image for a standard observer under typical viewing conditions. Specifically, the coefficients are designed to take into account the fact that the human eye is most sensitive to green light, followed by red and then blue light.\n",
    "\n",
    "The specific values of 0.299, 0.587, and 0.114 were chosen to provide a good balance between accuracy and simplicity, and have become widely adopted as a standard for grayscale conversion in BGR images."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1.3 Display Grayscale Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv2.imshow(\"Original Image\", img)\n",
    "cv2.imshow(\"Grayscale Image\", grayImg)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **<font style=\"color:rgb(0,255,0)\">Full Code</font>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "path = \"C:\\\\Users\\\\hp\\\\Google Drive\\\\Fiverr Work\\\\2022\\\\33. Computer Vision Course\\\\pictures\"\n",
    "\n",
    "img = cv2.imread(path+\"\\\\coloredChips.png\")  # took path and name of image as an argument\n",
    "\n",
    "blueChannel = img[:,:,0]\n",
    "greenChannel = img[:,:,1]\n",
    "redChannel = img[:,:,2]\n",
    "\n",
    "grayImg = (0.299 * redChannel + 0.587 * greenChannel + 0.114 * blueChannel)/255\n",
    "\n",
    "cv2.imshow(\"Original Image\", img)\n",
    "cv2.imshow(\"Grayscale Image\", grayImg)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "******"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **<font style=\"color:rgb(255,0,255)\">02. Use Mobile Phone Camera as Webcam</font>**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Download app iriun Webcam app from play store into your mobile \n",
    "https://play.google.com/store/apps/details?id=com.jacksoftw.webcam&hl=en&gl=US\n",
    "\n",
    "* Also download software iriun for windows into your laptop https://iriun.com/"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **<font style=\"color:rgb(0,255,0)\">Full Code</font>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import cv2\n",
    "\n",
    "cam = cv2.VideoCapture(0)\n",
    "\n",
    "while True:\n",
    "    Success, Frame = cam.read()\n",
    "\n",
    "    cv2.imshow(\"Mobile Cam\", Frame)\n",
    "    k = cv2.waitKey(1)\n",
    "\n",
    "    if k == ord(\"q\"):\n",
    "        break\n",
    "\n",
    "cam.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***********"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **<font style=\"color:rgb(255,0,255)\">03. Screen Recorder Using OpenCV</font>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import pyautogui\n",
    "import numpy as np"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3.1 Get Screen Resolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create resolution\n",
    "screenResolution = pyautogui.size()     # return screen width and height"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3.2 Get the file Name and Path where you want to store recording"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# file name in which we want to store recording\n",
    "fileName = input(\"Enter file name and path: \")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3.3 Define fourcc"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "cv2.VideoWriter_fourcc is a function in the OpenCV library for Python that is used to create a four-character code (fourcc) that specifies the video codec to be used when writing a video file using cv2.VideoWriter.\n",
    "\n",
    "The fourcc code is a 32-bit integer that is used to identify the video codec used to encode the video frames. Different video codecs have different fourcc codes, and the fourcc code can be used to specify the codec when opening a video file for writing.\n",
    "\n",
    "The function takes four arguments, which are the four characters that make up the fourcc code. These characters can be any ASCII characters, and they are combined into a 32-bit integer using bitwise operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now fix the frame rate\n",
    "fps = 10\n",
    "\n",
    "fourcc = cv2.VideoWriter_fourcc(*'XVID')\n",
    "output = cv2.VideoWriter(fileName, fourcc, fps, screenResolution)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3.4 Creating Recording Window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create recording module\n",
    "cv2.namedWindow(\"Live Recording\", cv2.WINDOW_NORMAL)\n",
    "cv2.resizeWindow(\"Live Recording\", (640, 480))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3.5 Start Recording"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "while True:\n",
    "    img = pyautogui.screenshot()\n",
    "    f = np.array(img)\n",
    "\n",
    "    f = cv2.cvtColor(f, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    output.write(f)\n",
    "    cv2.imshow(\"Live Recording\", f)\n",
    "\n",
    "    k = cv2.waitKey(1)\n",
    "\n",
    "    if k == ord(\"q\"):\n",
    "        break\n",
    "\n",
    "output.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **<font style=\"color:rgb(0,255,0)\">Full Code</font>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Screen Recoder\n",
    "\n",
    "import cv2\n",
    "import pyautogui\n",
    "import numpy as np\n",
    "\n",
    "# create resolution\n",
    "screenResolution = pyautogui.size()     # return screen width and height\n",
    "\n",
    "# file name in which we want to store recording\n",
    "fileName = input(\"Enter file name and path: \")\n",
    "\n",
    "# Now fix the frame rate\n",
    "fps = 30\n",
    "\n",
    "fourcc = cv2.VideoWriter_fourcc(*'XVID')\n",
    "output = cv2.VideoWriter(fileName, fourcc, fps, screenResolution)\n",
    "\n",
    "# create recording module\n",
    "cv2.namedWindow(\"Live Recording\", cv2.WINDOW_NORMAL)\n",
    "cv2.resizeWindow(\"Live Recording\", (640, 480))\n",
    "\n",
    "while True:\n",
    "    img = pyautogui.screenshot()\n",
    "    f = np.array(img)\n",
    "\n",
    "    f = cv2.cvtColor(f, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    output.write(f)\n",
    "    cv2.imshow(\"Live Recording\", f)\n",
    "\n",
    "    k = cv2.waitKey(1)\n",
    "\n",
    "    if k == ord(\"q\"):\n",
    "        break\n",
    "\n",
    "output.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*******"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **<font style=\"color:rgb(255,0,255)\">04. Extracting Frames from Video or Webcam</font>**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **<font style=\"color:rgb(0,255,0)\">Full Code</font>**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4.1 Save on pressing **<font style=\"color:rgb(0,255,0)\">\"S\"</font>** button"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "cam = cv2.VideoCapture(\"traffic.avi\")\n",
    "\n",
    "count = 0\n",
    "\n",
    "while True:\n",
    "\n",
    "    Success, frame = cam.read()\n",
    "\n",
    "    if Success:\n",
    "        \n",
    "        # # setting the frame speed\n",
    "        # cam.set(cv2.CAP_PROP_POS_MSEC, (count**100))\n",
    "\n",
    "        cv2.imshow(\"frame extraction\", frame)\n",
    "\n",
    "        k = cv2.waitKey(1)\n",
    "\n",
    "        if k == ord('q'):\n",
    "            cv2.destroyAllWindows()\n",
    "            break\n",
    "        elif k == ord(\"s\"):\n",
    "            cv2.imwrite(f\"frames\\\\imgn{count}.jpg\", frame)\n",
    "            count +=1\n",
    "    else:\n",
    "        # If the video reaches the end, set the frame position back to the beginning\n",
    "        cam.set(cv2.CAP_PROP_POS_FRAMES, 0)\n",
    "            \n",
    "\n",
    "cam.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "cam = cv2.VideoCapture(0)\n",
    "\n",
    "count = 0\n",
    "\n",
    "while True:\n",
    "\n",
    "    Success, frame = cam.read()\n",
    "\n",
    "    if Success:\n",
    "        \n",
    "        # # setting the frame speed\n",
    "        # cam.set(cv2.CAP_PROP_POS_MSEC, (count**100))\n",
    "\n",
    "        cv2.imshow(\"frame extraction\", frame)\n",
    "\n",
    "        k = cv2.waitKey(1)\n",
    "\n",
    "        if k == ord('q'):\n",
    "            cv2.destroyAllWindows()\n",
    "            break\n",
    "        elif k == ord(\"s\"):\n",
    "            cv2.imwrite(f\"frames\\\\imgn{count}.jpg\", frame)\n",
    "            count +=1\n",
    "            \n",
    "\n",
    "cam.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cam.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b671c20432fcd147198c92e7f072af9e705f087eb990bee22b07f08caab9f630"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
